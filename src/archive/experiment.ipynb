{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Occlusion for image segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "from captum.attr import Occlusion\n",
    "from copy import deepcopy\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# utils\n",
    "import importlib\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "import network\n",
    "from datasets import SegmentationDataset\n",
    "\n",
    "save_path = \"../maps/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_occlusion_map(model, input_image, mask=None, patch_size=5, stride=1):\n",
    "    \"\"\"\n",
    "    Generate an occlusion map for a given image input and specified mask region.\n",
    "\n",
    "    Args:\n",
    "    - model (torch.nn.Module): Pre-trained segmentation model.\n",
    "    - input_image (torch.Tensor): Input image tensor of shape (C, H, W).\n",
    "    - mask (torch.Tensor): Mask of shape (H, W), with 1s for pixels to analyze, 0s elsewhere.\n",
    "    - patch_size (int): The size of the occlusion patch.\n",
    "    - stride (int): Stride for moving the occlusion patch across the image.\n",
    "\n",
    "    Returns:\n",
    "    - occlusion_map (np.array): Heatmap of occlusion influence for the masked region.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure input image and model are on the same device\n",
    "    input_image = input_image.to(device).unsqueeze(0)  # add batch dimension\n",
    "    original_pred = model(input_image).squeeze(0)  # model prediction on original image\n",
    "    original_pred = original_pred.cpu().detach().numpy()\n",
    "    original_masked_pred = original_pred * mask  # Apply mask to the output\n",
    "\n",
    "    # Initialize occlusion map\n",
    "    occlusion_map = np.zeros(input_image.shape[2:])\n",
    "\n",
    "    # Iterate over image in patches\n",
    "    for y in tqdm(range(0, input_image.shape[2] - patch_size + 1, stride)):\n",
    "        for x in range(0, input_image.shape[3] - patch_size + 1, stride):\n",
    "            # Clone and occlude a patch in the input image\n",
    "            occluded_image = deepcopy(input_image)\n",
    "            occluded_image[:, :, y:y + patch_size, x:x + patch_size] = 0  # occlude with zeros\n",
    "\n",
    "            # Get prediction for the occluded image\n",
    "            with torch.no_grad():\n",
    "                occluded_pred = model(occluded_image).squeeze(0)\n",
    "                occluded_pred = occluded_pred.cpu().detach().numpy()\n",
    "\n",
    "            # Calculate difference in predictions for the masked region\n",
    "            occlusion_influence = np.abs(original_masked_pred - (occluded_pred * mask)).sum().item()\n",
    "            occlusion_map[y:y + patch_size, x:x + patch_size] += occlusion_influence\n",
    "\n",
    "    # Normalize occlusion map to [0, 1]\n",
    "    occlusion_map = (occlusion_map - np.min(occlusion_map)) / (np.max(occlusion_map) - np.min(occlusion_map))\n",
    "\n",
    "    return occlusion_map\n",
    "\n",
    "def plot_occlusion_map(bands, occlusion_map):\n",
    "    # Plotting the occlusion map\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    nir = bands[3].cpu().detach().numpy()\n",
    "    plt.imshow(nir, cmap='gray', alpha=0.5)\n",
    "    plt.imshow(occlusion_map, cmap='hot', alpha=0.6)\n",
    "    plt.colorbar(label=\"Influence Score\")\n",
    "    \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load paths\n",
    "paths = glob.glob('../../../data/LICS/test/*')\n",
    "print(len(paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.load(paths[0])\n",
    "utils.display_bands(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset object\n",
    "lics_dataset = SegmentationDataset(paths) \n",
    "\n",
    "# Download the model directly from Hugging Face\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"a-data-odyssey/coastal-image-segmentation\", \n",
    "    filename=\"LICS_UNET_12JUL2024.pth\")\n",
    "\n",
    "# Load the model\n",
    "model = torch.load(model_path,map_location=torch.device('cpu'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "device = torch.device('mps' if torch.backends.mps.is_built() \n",
    "                      else 'cuda' if torch.cuda.is_available() \n",
    "                      else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load first instance\n",
    "bands, target, edge = lics_dataset.__getitem__(0)\n",
    "\n",
    "# Make a prediction\n",
    "input = bands.to(device)\n",
    "input = input.unsqueeze(0)\n",
    "output = model(input)\n",
    "\n",
    "print(target.shape)\n",
    "print(output.shape)\n",
    "\n",
    "# Get the water mask \n",
    "target_water = np.argmax(target, axis=0)\n",
    "\n",
    "# Get the predicted water mask\n",
    "output = output.cpu().detach().numpy().squeeze()\n",
    "output = np.argmax(output, axis=0)\n",
    "\n",
    "# Plot the prediction\n",
    "fig, axs = plt.subplots(1, 2, figsize=(9, 5))\n",
    "axs[0].imshow(target_water, cmap='gray')\n",
    "axs[0].set_title('Target', fontsize=16)\n",
    "\n",
    "axs[1].imshow(output, cmap='gray')\n",
    "\n",
    "accuracy = np.mean(np.array(target_water == output))\n",
    "accuracy = round(accuracy, 3)\n",
    "axs[1].set_title(f'Prediction ({accuracy})', fontsize=16)\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = bands.to(device).unsqueeze(0)\n",
    "\n",
    "rgb_image = input_image.cpu().detach().numpy().squeeze()\n",
    "rgb_image = np.moveaxis(rgb_image, 0, -1)\n",
    "print(rgb_image.shape)\n",
    "rgb_image = rgb_image[:,:,0:3]\n",
    "rgb_image = np.clip(rgb_image, 0, 0.3)/0.3\n",
    "print(rgb_image.shape)\n",
    "\n",
    "\n",
    "plt.imshow(rgb_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occlusion_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = bands.to(device).unsqueeze(0)\n",
    "n_channels, height, width = input_image.shape[1:]\n",
    "patch_size = 32\n",
    "\n",
    "occlusion_values = occlusion_values = torch.rand((1,n_channels,1,1)) # mean of each channel\n",
    "print(np.round(occlusion_values.cpu().detach().numpy(),2))\n",
    "\n",
    "# Create 32 x32 patch \n",
    "occlusion_patch = torch.zeros((1,n_channels,patch_size,patch_size))\n",
    "occlusion_patch += occlusion_values\n",
    "\n",
    "#occlusion_patch[:, :, :patch_size, :patch_size] = occlusion_values[:, :, None, None]\n",
    "\n",
    "print(occlusion_values.shape)\n",
    "print(occlusion_patch.shape)\n",
    "\n",
    "\n",
    "input_image[:, :, :patch_size, :patch_size] = occlusion_patch\n",
    "\n",
    "rgb_image = input_image.cpu().detach().numpy().squeeze()\n",
    "rgb_image = np.moveaxis(rgb_image, 0, -1)\n",
    "print(rgb_image.shape)\n",
    "rgb_image = rgb_image[:,:,0:3]\n",
    "rgb_image = np.clip(rgb_image, 0, 0.3)/0.3\n",
    "print(rgb_image.shape)\n",
    "\n",
    "plt.imshow(rgb_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load first instance\n",
    "bands, target, edge = lics_dataset.__getitem__(0)\n",
    "\n",
    "# Shape of the image\n",
    "print(bands.shape)\n",
    "print(target.shape)\n",
    "print(edge.shape)\n",
    "\n",
    "# visualise the image\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "rgb = utils.get_rgb(np.array(bands),contrast=0.3)\n",
    "\n",
    "ax[0].imshow(rgb)\n",
    "ax[1].imshow(target[1], cmap='gray')\n",
    "ax[2].imshow(edge, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_pixel = np.zeros_like(target[1])\n",
    "land_pixel[50, 50] = 1\n",
    "land_pixel[150, 5] = 1\n",
    "land_pixel[130, 150] = 1\n",
    "\n",
    "water_pixel = np.zeros_like(target[1])\n",
    "water_pixel[200, 200] = 1\n",
    "water_pixel[100,150] = 1\n",
    "water_pixel[200,50] = 1\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(10,5))\n",
    "\n",
    "ax[0].imshow(land_pixel,cmap='gray')\n",
    "ax[1].imshow(water_pixel,cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "bands, target, edge = lics_dataset.__getitem__(i)\n",
    "\n",
    "patch_size = 5\n",
    "stride = 1\n",
    "\n",
    "# Generate occlusion maps\n",
    "water_map = generate_occlusion_map(model, bands, water_pixel,patch_size=patch_size,stride=stride)\n",
    "np.save(save_path + f\"water_map_{i}_{patch_size}_{stride}.npy\", water_map)\n",
    "\n",
    "land_map = generate_occlusion_map(model, bands, land_pixel,patch_size=patch_size,stride=stride)\n",
    "np.save(save_path + f\"land_map_{i}_{patch_size}_{stride}.npy\", land_map)\n",
    "\n",
    "mask = edge.cpu().detach().numpy()\n",
    "coasline_map = generate_occlusion_map(model, bands, mask,patch_size=patch_size,stride=stride)\n",
    "np.save(save_path + f\"coastline_map_{i}_{patch_size}_{stride}.npy\", coasline_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_map = np.load(save_path + f\"water_map_0_10_10.npy\")\n",
    "plot_occlusion_map(bands, water_map)\n",
    "\n",
    "land_map = np.load(save_path + f\"land_map_0_10_10.npy\")\n",
    "plot_occlusion_map(bands, land_map)\n",
    "\n",
    "coasline_map = np.load(save_path + f\"coastline_map_0_10_10.npy\")\n",
    "plot_occlusion_map(bands, coasline_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load first instance\n",
    "bands, target, edge = lics_dataset.__getitem__(3)\n",
    "\n",
    "# Shape of the image\n",
    "print(bands.shape)\n",
    "print(target.shape)\n",
    "print(edge.shape)\n",
    "\n",
    "# visualise the image\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "rgb = utils.get_rgb(np.array(bands),contrast=0.3)\n",
    "\n",
    "ax[0].imshow(rgb)\n",
    "ax[1].imshow(target[1], cmap='gray')\n",
    "ax[2].imshow(edge, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_pixel = np.zeros_like(target[1])\n",
    "#land_pixel = np.copy(edge)\n",
    "land_pixel[230, 50] = 1\n",
    "land_pixel[230, 200] = 1\n",
    "land_pixel[50, 50] = 1\n",
    "\n",
    "water_pixel = np.zeros_like(target[1])\n",
    "water_pixel[50, 200] = 1\n",
    "water_pixel[150,120] = 1\n",
    "water_pixel[210,50] = 1\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(10,5))\n",
    "\n",
    "ax[0].imshow(land_pixel,cmap='gray')\n",
    "ax[1].imshow(water_pixel,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "bands, target, edge = lics_dataset.__getitem__(i)\n",
    "\n",
    "patch_size = 10\n",
    "stride = 10\n",
    "\n",
    "# Generate occlusion maps\n",
    "water_map = generate_occlusion_map(model, bands, water_pixel,patch_size=patch_size,stride=stride)\n",
    "np.save(save_path + f\"water_map_{i}_{patch_size}_{stride}.npy\", water_map)\n",
    "\n",
    "land_map = generate_occlusion_map(model, bands, land_pixel,patch_size=patch_size,stride=stride)\n",
    "np.save(save_path + f\"land_map_{i}_{patch_size}_{stride}.npy\", land_map)\n",
    "\n",
    "mask = edge.cpu().detach().numpy()\n",
    "coasline_map = generate_occlusion_map(model, bands, mask,patch_size=patch_size,stride=stride)\n",
    "np.save(save_path + f\"coastline_map_{i}_{patch_size}_{stride}.npy\", coasline_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_map = np.load(save_path + f\"water_map_3_10_10.npy\")\n",
    "plot_occlusion_map(bands, water_map)\n",
    "\n",
    "land_map = np.load(save_path + f\"land_map_3_10_10.npy\")\n",
    "plot_occlusion_map(bands, land_map)\n",
    "\n",
    "coasline_map = np.load(save_path + f\"coastline_map_3_10_10.npy\")\n",
    "plot_occlusion_map(bands, coasline_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load first instance\n",
    "bands, target, edge = lics_dataset.__getitem__(5)\n",
    "\n",
    "# Shape of the image\n",
    "print(bands.shape)\n",
    "print(target.shape)\n",
    "print(edge.shape)\n",
    "\n",
    "# visualise the image\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "rgb = utils.get_rgb(np.array(bands),contrast=0.3)\n",
    "\n",
    "ax[0].imshow(rgb)\n",
    "ax[1].imshow(target[1], cmap='gray')\n",
    "ax[2].imshow(edge, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_pixel = np.zeros_like(target[1])\n",
    "#land_pixel = np.copy(edge)\n",
    "land_pixel[200, 50] = 1\n",
    "land_pixel[50, 150] = 1\n",
    "land_pixel[50, 50] = 1\n",
    "\n",
    "water_pixel = np.zeros_like(target[1])\n",
    "#water_pixel = np.copy(edge)\n",
    "water_pixel[50, 200] = 1\n",
    "water_pixel[150,120] = 1\n",
    "water_pixel[200,200] = 1\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(10,5))\n",
    "\n",
    "ax[0].imshow(land_pixel,cmap='gray')\n",
    "ax[1].imshow(water_pixel,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 5\n",
    "bands, target, edge = lics_dataset.__getitem__(i)\n",
    "\n",
    "patch_size = 10\n",
    "stride = 10\n",
    "\n",
    "# Generate occlusion maps\n",
    "water_map = generate_occlusion_map(model, bands, water_pixel,patch_size=patch_size,stride=stride)\n",
    "np.save(save_path + f\"water_map_{i}_{patch_size}_{stride}.npy\", water_map)\n",
    "\n",
    "land_map = generate_occlusion_map(model, bands, land_pixel,patch_size=patch_size,stride=stride)\n",
    "np.save(save_path + f\"land_map_{i}_{patch_size}_{stride}.npy\", land_map)\n",
    "\n",
    "mask = edge.cpu().detach().numpy()\n",
    "coasline_map = generate_occlusion_map(model, bands, mask,patch_size=patch_size,stride=stride)\n",
    "np.save(save_path + f\"coastline_map_{i}_{patch_size}_{stride}.npy\", coasline_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_map = np.load(save_path + f\"water_map_5_10_10.npy\")\n",
    "plot_occlusion_map(bands, water_map)\n",
    "\n",
    "land_map = np.load(save_path + f\"land_map_5_10_10.npy\")\n",
    "plot_occlusion_map(bands, land_map)\n",
    "\n",
    "coasline_map = np.load(save_path + f\"coastline_map_5_10_10.npy\")\n",
    "plot_occlusion_map(bands, coasline_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = edge.cpu().detach().numpy()\n",
    "# Ensure input image and model are on the same device\n",
    "input_image = bands.to(device).unsqueeze(0)  # add batch dimension\n",
    "original_pred = model(input_image).squeeze(0)  # model prediction on original image\n",
    "original_pred = original_pred.cpu().detach().numpy()\n",
    "original_masked_pred = original_pred * mask  # Apply mask to the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(original_pred.shape)\n",
    "print(original_masked_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize occlusion map\n",
    "occlusion_map = np.zeros(input_image.shape[2:])\n",
    "\n",
    "print(occlusion_map.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 0\n",
    "x = 0\n",
    "patch_size = 10\n",
    "occluded_image = deepcopy(input_image)\n",
    "occluded_image[:, :, y:y + patch_size, x:x + patch_size] = 0  # occlude with zeros\n",
    "\n",
    "nir = occluded_image[0][3].cpu().detach().numpy()\n",
    "plt.imshow(nir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    occluded_pred = model(occluded_image).squeeze(0)\n",
    "    occluded_pred = occluded_pred.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differnece = np.abs(original_masked_pred - (occluded_pred * mask))\n",
    "occlusion_influence = differnece.sum().item()\n",
    "\n",
    "print(occluded_pred.shape)\n",
    "print(differnece.shape)\n",
    "print(occlusion_influence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over image in patches\n",
    "for y in tqdm(range(0, input_image.shape[2] - patch_size + 1, stride)):\n",
    "    for x in range(0, input_image.shape[3] - patch_size + 1, stride):\n",
    "        # Clone and occlude a patch in the input image\n",
    "        \n",
    "        # Get prediction for the occluded image\n",
    "        \n",
    "\n",
    "        # Calculate difference in predictions for the masked region\n",
    "        \n",
    "        occlusion_map[y:y + patch_size, x:x + patch_size] += occlusion_influence\n",
    "\n",
    "# Normalize occlusion map to [0, 1]\n",
    "occlusion_map = (occlusion_map - np.min(occlusion_map)) / (np.max(occlusion_map) - np.min(occlusion_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate occlusion map using Captum\n",
    "occlusion_map = generate_occlusion_map(model, bands, land_pixel,patch_size=5,stride=5)\n",
    "\n",
    "# Plotting the occlusion map\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(rgb, cmap='gray', alpha=0.5)\n",
    "plt.imshow(occlusion_map, cmap='hot', alpha=0.6)\n",
    "plt.colorbar(label=\"Influence Score\")\n",
    "plt.title(\"Occlusion Map - Influence on Predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate occlusion map using Captum\n",
    "coastline = edge.cpu().detach().numpy()\n",
    "occlusion_map = generate_occlusion_map(model, bands, coastline,patch_size=5,stride=5)\n",
    "\n",
    "# Plotting the occlusion map\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(rgb, cmap='gray', alpha=0.5)\n",
    "plt.imshow(occlusion_map, cmap='hot', alpha=0.6)\n",
    "plt.colorbar(label=\"Influence Score\")\n",
    "plt.title(\"Occlusion Map - Influence on Predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "\n",
    "# Load and prepare the data\n",
    "input_image, target = lics_dataset.__getitem__(0)\n",
    "mask = torch.zeros_like(target, dtype=torch.float32)\n",
    "mask[target == 1] = 1  # Example: focus on ocean pixels\n",
    "\n",
    "# Generate occlusion map for masked pixels\n",
    "occlusion_map = generate_occlusion_map(model, input_image, mask)\n",
    "\n",
    "# Plotting the occlusion map\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(to_pil_image(input_image), cmap='gray', alpha=0.5)\n",
    "plt.imshow(occlusion_map, cmap='hot', alpha=0.6)\n",
    "plt.colorbar(label=\"Influence Score\")\n",
    "plt.title(\"Occlusion Map - Influence on Predictions\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from captum.attr import Occlusion\n",
    "from skimage.segmentation import slic\n",
    "from skimage.color import label2rgb\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "def generate_superpixel_occlusion_map(model, input_image, mask=None, n_segments=50):\n",
    "    \"\"\"\n",
    "    Generate an occlusion map using superpixels as occlusion regions.\n",
    "    \n",
    "    Args:\n",
    "    - model (torch.nn.Module): Pre-trained segmentation model.\n",
    "    - input_image (torch.Tensor): Input image tensor of shape (C, H, W).\n",
    "    - mask (torch.Tensor): Mask of shape (H, W), with 1s for pixels to analyze, 0s elsewhere.\n",
    "    - n_segments (int): Number of superpixels to generate.\n",
    "    \n",
    "    Returns:\n",
    "    - occlusion_map (np.array): Heatmap of occlusion influence for the masked region.\n",
    "    \"\"\"\n",
    "    # Convert input tensor to numpy for superpixel segmentation\n",
    "    input_image_np = input_image.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    # Generate superpixels\n",
    "    superpixels = slic(input_image_np, n_segments=n_segments, start_label=0)\n",
    "    num_superpixels = superpixels.max() + 1\n",
    "    \n",
    "    # Initialize occlusion map\n",
    "    occlusion_map = np.zeros(superpixels.shape)\n",
    "    \n",
    "    # Define baseline as a zero image (same size as input)\n",
    "    baseline = torch.zeros_like(input_image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Loop over each superpixel to occlude it\n",
    "    for sp in range(num_superpixels):\n",
    "        # Create a mask for the current superpixel\n",
    "        sp_mask = torch.tensor(superpixels == sp, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Occlude the superpixel region by setting it to the baseline\n",
    "        occluded_image = input_image.clone().unsqueeze(0).to(device)\n",
    "        occluded_image[0][:, sp_mask.bool()] = 0  # Occlude with zero baseline\n",
    "\n",
    "        # Make predictions for occluded image\n",
    "        with torch.no_grad():\n",
    "            original_pred = model(input_image.unsqueeze(0).to(device)).squeeze(0) * mask.to(device)\n",
    "            occluded_pred = model(occluded_image).squeeze(0) * mask.to(device)\n",
    "        \n",
    "        # Calculate the influence score for this superpixel\n",
    "        influence_score = torch.abs(original_pred - occluded_pred).sum().item()\n",
    "        occlusion_map[superpixels == sp] = influence_score\n",
    "    \n",
    "    # Normalize occlusion map to [0, 1]\n",
    "    occlusion_map = (occlusion_map - np.min(occlusion_map)) / (np.max(occlusion_map) - np.min(occlusion_map))\n",
    "    \n",
    "    return occlusion_map\n",
    "\n",
    "# Load and prepare the data\n",
    "input_image, target = lics_dataset.__getitem__(0)\n",
    "mask = torch.zeros_like(target, dtype=torch.float32)\n",
    "mask[target == 1] = 1  # Example: focus on ocean pixels\n",
    "\n",
    "# Generate occlusion map using superpixels\n",
    "occlusion_map = generate_superpixel_occlusion_map(model, input_image, mask)\n",
    "\n",
    "# Plotting the occlusion map with superpixel boundaries\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(label2rgb(superpixels, input_image.permute(1, 2, 0).cpu().numpy(), kind='avg'), alpha=0.5)\n",
    "plt.imshow(occlusion_map, cmap='hot', alpha=0.6)\n",
    "plt.colorbar(label=\"Influence Score\")\n",
    "plt.title(\"Superpixel Occlusion Map - Influence on Predictions\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_occlusion_map(model, input_image, mask=None, sliding_window=5, stride=1):\n",
    "    \"\"\"\n",
    "    Generate an occlusion map using Captum for a given input image and mask.\n",
    "    \n",
    "    Args:\n",
    "    - model (torch.nn.Module): Pre-trained segmentation model.\n",
    "    - input_image (torch.Tensor): Input image tensor of shape (C, H, W).\n",
    "    - mask (torch.Tensor): Mask of shape (H, W), with 1s for pixels to analyze, 0s elsewhere.\n",
    "    - sliding_window (int): Size of the occlusion patch.\n",
    "    - stride (int): Stride for moving the occlusion patch across the image.\n",
    "    \n",
    "    Returns:\n",
    "    - occlusion_map (np.array): Heatmap of occlusion influence for the masked region.\n",
    "    \"\"\"\n",
    "    # Ensure input image and model are on the same device\n",
    "    input_image = input_image.to(device).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Initialize Captum Occlusion object\n",
    "    occlusion = Occlusion(model)\n",
    "    \n",
    "    # Generate occlusion attributions for the specified output class (0 or 1 for land or ocean)\n",
    "    attributions = occlusion.attribute(\n",
    "        input_image,\n",
    "        target=0,  # Index for land/ocean prediction, adjust as needed\n",
    "        sliding_window_shapes=(input_image.shape[1], sliding_window, sliding_window),\n",
    "        strides=(input_image.shape[1], stride, stride),\n",
    "        baselines=0\n",
    "    )\n",
    "    \n",
    "    # Convert attributions to numpy and apply mask to occlude only specified areas\n",
    "    occlusion_map = attributions.squeeze().cpu().numpy()\n",
    "    if mask is not None:\n",
    "        occlusion_map = occlusion_map * mask.cpu().numpy()\n",
    "    \n",
    "    # Normalize occlusion map to [0, 1]\n",
    "    occlusion_map = (occlusion_map - np.min(occlusion_map)) / (np.max(occlusion_map) - np.min(occlusion_map))\n",
    "    \n",
    "    return occlusion_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Occlusion",
   "language": "python",
   "name": "occlusion_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
